{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBS Maltese Food Recipes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed to get the html from a site and to parse the html\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "from parse_ingredients import parse_ingredient\n",
    "import pyfood as pyf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we need the main url. This is the site we will be scraping for recipes. Using the requests package we can load the website. This prepares it for scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the url to a variable, getting the url and parsing the html\n",
    "mainURL = \"https://www.sbs.com.au/food/cuisine/maltese\"\n",
    "req = requests.get(mainURL)\n",
    "soup = bs(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base link is the common part of all recipe links. The recipes will be added onto this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base link that hrefs will be added onto\n",
    "baseLink = 'https://www.sbs.com.au'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Beautiful soup package to crawl the htlm of the website for any instance of the class 'link-underlay'. This function will return the object holding that class. We determined that all the a tags on the site have this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag that stores the href to the recipes\n",
    "hrefs = soup.findAll(class_='link-underlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simply printing out the contents to see what the hrefs of the tags are\n",
    "\n",
    "def printHrefs(hrefsIn):\n",
    "    for i in hrefsIn:\n",
    "        print(i.get('href'), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare these links so that they can be accessed. To do this we append the base link to the start of the scraped link. This gives us an addressable link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for loop will generate the links to all the recipes that are shown on the mainURL\n",
    "\n",
    "def funcRecipeLinks(hrefsIn, baseLinkIn, recipeLinks):\n",
    "    for i in hrefsIn:\n",
    "        fullLink = baseLinkIn + i.get('href')\n",
    "        if i.get('href')[6] == 'r':\n",
    "            recipeLinks.append(fullLink)\n",
    "    return recipeLinks\n",
    "\n",
    "recipeLinks = []\n",
    "recipeLinks = funcRecipeLinks(hrefs, baseLink, recipeLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific site has 2 pages of recipes. Therefore, below we are going trough the same process as above for the other page. In the end we have the variable 'recipeLinks' that stores all the links (that are related to recipes) on the sbs website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainURL = \"https://www.sbs.com.au/food/cuisine/maltese?sort_by=created&page=1\"\n",
    "req = requests.get(mainURL)\n",
    "soup = bs(req.text, \"html.parser\")\n",
    "\n",
    "baseLink = 'https://www.sbs.com.au'\n",
    "hrefs = soup.findAll(class_='link-underlay')\n",
    "\n",
    "recipeLinks = funcRecipeLinks(hrefs, baseLink, recipeLinks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different ingredient extraction methods\n",
    "---\n",
    "### Method 1: parse_ingredients library\n",
    "\n",
    "The parse_ingredients library provides a way to extract ingredients, quantity, units and comments from recipe ingredients. In testing it was found that the library can be inconsistent with removing quantifiers such as size, colour and plurals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseIngredientsFunc(i, title):\n",
    "    data = []\n",
    "    for j in title:\n",
    "                data.append(str(j.get_text(strip=False))) # Dish Name\n",
    "    \n",
    "    parseResult = parse_ingredient(str(i.get_text(strip=False)))\n",
    "            \n",
    "    data.append(parseResult.original_string) #Original String\n",
    "    data.append(parseResult.name) #Ingredient\n",
    "    data.append(parseResult.quantity) #Quantity\n",
    "    data.append(parseResult.unit) #Unit\n",
    "    data.append(parseResult.comment) #Comment\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: pyfood library\n",
    "\n",
    "Contrary to the parse_ingredient library, the pyfood library only extracts the ingredient from the input string. Moreover, It is prone to some silly errors where it will convert ingredients like 'frozen peas' to 'green peppers'. These mistakes appear to be few and far between. However, examination shows a better handling of quantifiers; it ignores indicators to size and colour, as well as removes plurals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyfoodFunc(title, i):\n",
    "    data = []\n",
    "\n",
    "    for j in title:\n",
    "        data.append(str(j.get_text(strip=False))) # Dish Name\n",
    "        \n",
    "    print(str(i.get_text(strip=False)))\n",
    "    results = shelf.process_ingredients([i.get_text(strip=False)])\n",
    "    try:\n",
    "        temp = results['ingredients'][0]['foodname'] # vegetarian, vegan, nutrition, seasonality\n",
    "    except:\n",
    "        temp = results['HS'][0]\n",
    "    data.append(i)\n",
    "    data.append(temp)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: parse_ingredients and pyfood libraries\n",
    "\n",
    "After taking into consideration the above scenario, we decided to use both of these libraries for the best result. First, the parse_ingredients library is used to extract: quantity, and the ingredient name. The ingredient name is then passed trough the pyfoods function set to obtain a striped down version of the ingredient. Using the parse_ingredients library first also appears to reduce the error in the pyfoods library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseIngredientsAndPyfoodFunc(i, title, shelf):\n",
    "    data = []\n",
    "    for j in title:\n",
    "                data.append(str(j.get_text(strip=False))[0:-34]) # Dish Name\n",
    "    \n",
    "    parseResult = parse_ingredient(str(i.get_text(strip=False)))\n",
    "\n",
    "    results = shelf.process_ingredients([parseResult.name])\n",
    "    try:\n",
    "        temp = results['ingredients'][0]['foodname']\n",
    "    except:\n",
    "        temp = results['HS'][0]\n",
    "\n",
    "    temp = temp.replace(\" \", \"_\")\n",
    "\n",
    "    data.append(temp)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ingredientList = []\n",
    "shelf = pyf.Shelf(region='Italy', month_id=0)\n",
    "\n",
    "for recipe in recipeLinks:\n",
    "    req = requests.get(recipe) # Accesses the next recipe\n",
    "    soup = bs(req.text, \"html.parser\")\n",
    "    title = soup.findAll('h1') # finds the name of the recipe\n",
    "    \n",
    "    ingredientsDiv = soup.findAll('div', class_='field-name-field-ingredients') # finds all the divs containing the ingredients\n",
    "    \n",
    "    # For each div go trough and extract the ingredients\n",
    "    for ul in ingredientsDiv:\n",
    "        ingredients = ul.findAll(\"li\")\n",
    "        \n",
    "        for i in ingredients:\n",
    "            # data = parseIngredientsFunc(i, title)\n",
    "            # data = pyfoodFunc(title, i)\n",
    "            data = parseIngredientsAndPyfoodFunc(i, title, shelf)\n",
    "            \n",
    "            ingredientList.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding each ingredients type and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_info = pd.read_csv(\"../CSV/Compound CSVs/ingr_info.tsv\", sep=\"\\t\")\n",
    "\n",
    "category = [None] * len(ingredientList)\n",
    "index = [None] * len(ingredientList)\n",
    "\n",
    "for ing in range(0, len(ingredientList)):\n",
    "    for ing2 in range(0, len(ing_info[\"ingredient name\"])):\n",
    "        if(ingredientList[ing][1] == ing_info[\"ingredient name\"][ing2]):\n",
    "            category[ing] = ing_info[\"category\"][ing2]\n",
    "            index[ing] = ing_info[\"# id\"][ing2]\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"Recipe\", \"Ingredient\"], data = ingredientList)\n",
    "\n",
    "df.insert(2, \"Ingredint Index\", index, True)\n",
    "df.insert(2, \"Ingredint Category\", category, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data to a CSV\n",
    "\n",
    "It was decided to save the scraped data to a series of CSVs. This was done to make the data easier and faster to access as you do not need to wait for the long scraping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToCSV(header, rows, filePath):\n",
    "    with open(filePath, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # write the header\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # write multiple rows\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following csv stores all the ingredients for each dish found on the sbs website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../CSV/recipeList.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a list of recipes and there links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following for loop looks over all the links previously scraped, gets the title from each page (this will indicate the recipe name), and saves the recipe name and link to csv. This provides us with a list of recipes and there websites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecipeList = []\n",
    "\n",
    "for recipe in recipeLinks:\n",
    "    req = requests.get(recipe) # Accesses the next recipe\n",
    "    soup = bs(req.text, \"html.parser\")\n",
    "    title = soup.findAll('h1') # finds the name of the recipe\n",
    "\n",
    "    row = []\n",
    "\n",
    "    for j in title:\n",
    "        row.append(str(j.get_text(strip=False)))\n",
    "\n",
    "    row.append(recipe)\n",
    "\n",
    "    RecipeList.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Recipe Name', 'Links']\n",
    "\n",
    "saveToCSV(header, RecipeList, '../CSV/recipeLinks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
